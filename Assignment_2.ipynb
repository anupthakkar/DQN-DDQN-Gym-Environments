{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from matplotlib.offsetbox import AnnotationBbox, OffsetImage\n",
    "from operator import add\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "plt.rcParams[\"figure.figsize\"] = (20,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridEnvironment(gym.Env):\n",
    "    \n",
    "    def __init__(self, type):                     #type variable defines the environment type ie deterministic or stochastic\n",
    "        self.observation_space = spaces.Discrete(25)\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.max_timesteps = 50\n",
    "        self.reward_action_step = -0.5          #reward associated with each timestep\n",
    "        self.outbound_reward = -1             #each time the agent tries to go out of the environment it gets negative reward\n",
    "        self.env_type = type\n",
    "        print('***************{}***************'.format(self.env_type+' environment'))\n",
    "        \n",
    "    def reset(self):\n",
    "        self.timestep = 0\n",
    "        self.reward = 0\n",
    "        self.done = False\n",
    "        self.agent_pos = [0,0]\n",
    "        self.intermediate_goal1 = [2,1]\n",
    "        self.intermediate_goal2 = [0,4]\n",
    "        self.intermediate_goal3 = [4,0]\n",
    "        self.final_goal_pos = [4,3]\n",
    "        self.monster_pos = [2,3]\n",
    "        self.pit_pos = [3,1]\n",
    "        self.state = np.zeros((5, 5))\n",
    "        observation = self.state.flatten()\n",
    "        \n",
    "        self.reward_dict = {                         #dict to store the values of rewards\n",
    "            'intermediate_goal1': 2,\n",
    "            'intermediate_goal2': 5,\n",
    "            'intermediate_goal3': 2,\n",
    "            'final_goal': 25,\n",
    "            'monster': -10,\n",
    "            'pit': -5\n",
    "        }\n",
    "        self.visited_dict = {                        #dict to store the visited status \n",
    "            'intermediate_goal1': 0,\n",
    "            'intermediate_goal2': 0,\n",
    "            'intermediate_goal3': 0,\n",
    "            'final_goal_pos': 0,\n",
    "            'monster_pos': 0,\n",
    "            'pit_pos': 0\n",
    "        }\n",
    "        \n",
    "        return observation\n",
    "    \n",
    "    def step(self,action):\n",
    "\n",
    "        current_reward = self.reward_action_step\n",
    "        self.timestep += 1\n",
    "        \n",
    "        #defining the stochastic part of the environment\n",
    "        if(self.env_type == 'stochastic'):\n",
    "            if(action == 0):\n",
    "                action = np.random.choice(4, 1, p=[0.95, 0.03, 0.01, 0.01])[0]\n",
    "            elif(action == 1):\n",
    "                action = np.random.choice(4, 1, p=[0.03, 0.95, 0.01, 0.01])[0]\n",
    "            elif(action == 2):\n",
    "                action = np.random.choice(4, 1, p=[0.01, 0.01, 0.95, 0.03])[0]\n",
    "            else:\n",
    "                action = np.random.choice(4, 1, p=[0.01, 0.01, 0.03, 0.95])[0]\n",
    "                \n",
    "#         print('Action that happened: {}'.format(self.action_values(action)))\n",
    "    \n",
    "        old_pos = self.agent_pos.copy()\n",
    "        \n",
    "        if action == 0:\n",
    "            self.agent_pos[1] -= 1 #down\n",
    "        if action == 1:\n",
    "            self.agent_pos[1] += 1 #up\n",
    "        if action == 2:\n",
    "            self.agent_pos[0] -= 1 #left\n",
    "        if action == 3:\n",
    "            self.agent_pos[0] += 1 #right\n",
    "        \n",
    "        if(self.agent_pos[0] > 4 or self.agent_pos[1] > 4 or self.agent_pos[0] < 0 or self.agent_pos[1] < 0):\n",
    "            current_reward += self.outbound_reward\n",
    "        self.agent_pos = np.clip(self.agent_pos, 0, 4)         #clip function to ensure safety of the agent\n",
    "        self.state = np.zeros((5,5))\n",
    "        \n",
    "        next_action_possible = 1                            #defines whether the next action is possible or not\n",
    "                                                            # 1 if possible 0 if goal reached and -1 if dead by monster or in pit\n",
    "        \n",
    "        if(self.agent_pos == self.intermediate_goal1).all():\n",
    "            if(self.visited_dict['intermediate_goal1'] == 0):\n",
    "                current_reward += self.reward_dict['intermediate_goal1']\n",
    "            self.visited_dict['intermediate_goal1'] += 1\n",
    "            \n",
    "        if(self.agent_pos == self.intermediate_goal2).all():\n",
    "            if(self.visited_dict['intermediate_goal2'] == 0):\n",
    "                current_reward += self.reward_dict['intermediate_goal2']\n",
    "            self.visited_dict['intermediate_goal2'] += 1\n",
    "            \n",
    "        if(self.agent_pos == self.intermediate_goal3).all():\n",
    "            if(self.visited_dict['intermediate_goal3'] == 0):\n",
    "                current_reward += self.reward_dict['intermediate_goal3']\n",
    "            self.visited_dict['intermediate_goal3'] += 1\n",
    "        \n",
    "        if (self.agent_pos == self.final_goal_pos).all():\n",
    "            current_reward += self.reward_dict['final_goal']\n",
    "            next_action_possible = 0\n",
    "            self.visited_dict['final_goal_pos'] += 1\n",
    "        \n",
    "        self.done = True if(self.timestep >= self.max_timesteps or (self.agent_pos == self.final_goal_pos).all())else False\n",
    "        \n",
    "        if(self.agent_pos == self.monster_pos).all():\n",
    "            self.visited_dict['monster_pos'] += 1\n",
    "            current_reward += self.reward_dict['monster']\n",
    "            self.done = True\n",
    "            next_action_possible = -2\n",
    "        \n",
    "        if(self.agent_pos == self.pit_pos).all():\n",
    "            self.visited_dict['pit_pos'] += 1\n",
    "            current_reward += self.reward_dict['pit']\n",
    "            self.done = True\n",
    "            next_action_possible = -1\n",
    "        \n",
    "        self.reward += current_reward\n",
    "        info = {'next_action_possible': next_action_possible, 'current_agent_pos': self.agent_pos}\n",
    "        self.state[tuple(self.agent_pos)] = 1\n",
    "        observation = self.state.flatten()\n",
    "        return observation, current_reward, self.done, info\n",
    "    \n",
    "    def action_values(self, action):                                      #method to give the name of the action\n",
    "        a = ''\n",
    "        if(action == 0):\n",
    "            a = 'Down'\n",
    "        if(action == 1):\n",
    "            a = 'Up'\n",
    "        if(action == 2):\n",
    "            a = 'Left'\n",
    "        if(action == 3):\n",
    "            a = 'Right'\n",
    "        return a\n",
    "    \n",
    "    def render(self):\n",
    "        fix ,ax = plt.subplots(figsize=(10,10))\n",
    "        ax.set_xlim(0,5)\n",
    "        ax.set_ylim(0,5)\n",
    "\n",
    "        agent = AnnotationBbox(OffsetImage(plt.imread('./images/agent.png'), zoom=0.20),  # Plotting the agent.\n",
    "                       list(map(add, self.agent_pos, [0.5, 0.5])), frameon=False)\n",
    "        if(self.agent_pos[0] == self.intermediate_goal1[0] and self.agent_pos[1] == self.intermediate_goal1[1]):\n",
    "            if(self.visited_dict['intermediate_goal1'] == 1):\n",
    "                agent = AnnotationBbox(OffsetImage(plt.imread('./images/agent_reward.png'), zoom=0.15),  # Plotting the agent with reward.\n",
    "                           list(map(add, self.agent_pos, [0.5, 0.5])), frameon=False)\n",
    "        elif(self.agent_pos[0] == self.intermediate_goal3[0] and self.agent_pos[1] == self.intermediate_goal3[1]):\n",
    "            if(self.visited_dict['intermediate_goal3'] == 1):\n",
    "                agent = AnnotationBbox(OffsetImage(plt.imread('./images/agent_reward.png'), zoom=0.15),  # Plotting the agent with reward.\n",
    "                           list(map(add, self.agent_pos, [0.5, 0.5])), frameon=False)\n",
    "        elif(self.agent_pos[0] == self.intermediate_goal2[0] and self.agent_pos[1] == self.intermediate_goal2[1]):\n",
    "            if(self.visited_dict['intermediate_goal2'] == 1):\n",
    "                agent = AnnotationBbox(OffsetImage(plt.imread('./images/agent_reward.png'), zoom=0.15),  # Plotting the agent with reward.\n",
    "                           list(map(add, self.agent_pos, [0.5, 0.5])), frameon=False)\n",
    "        elif(self.agent_pos[0] == self.pit_pos[0] and self.agent_pos[1] == self.pit_pos[1]):\n",
    "            agent = AnnotationBbox(OffsetImage(plt.imread('./images/agent_in_pit.png'), zoom=0.08),  # Plotting the agent in pit.\n",
    "                       list(map(add, self.agent_pos, [0.5, 0.5])), frameon=False)\n",
    "        elif(self.agent_pos[0] == self.final_goal_pos[0] and self.agent_pos[1] == self.final_goal_pos[1]):\n",
    "            agent = AnnotationBbox(OffsetImage(plt.imread('./images/agent_goal.png'), zoom=0.18),  # Plotting the agent with goal.\n",
    "                       list(map(add, self.agent_pos, [0.5, 0.5])), frameon=False)\n",
    "        elif(self.agent_pos[0] == self.monster_pos[0] and self.agent_pos[1] == self.monster_pos[1]):\n",
    "            agent = AnnotationBbox(OffsetImage(plt.imread('./images/dead_agent.png'), zoom=0.18),  # Plotting the dead agent.\n",
    "                       list(map(add, self.agent_pos, [0.5, 0.5])), frameon=False)\n",
    "        ax.add_artist(agent)\n",
    "        \n",
    "        pit = AnnotationBbox(OffsetImage(plt.imread('./images/pit.png'), zoom=0.10),  # Plotting the pit.\n",
    "                       list(map(add, self.pit_pos, [0.5, 0.5])), frameon=False)\n",
    "        if(self.visited_dict['pit_pos'] == 0):\n",
    "            ax.add_artist(pit)\n",
    "        \n",
    "        monster = AnnotationBbox(OffsetImage(plt.imread('./images/monster.png'), zoom=0.12),  # Plotting the monster.\n",
    "                       list(map(add, self.monster_pos, [0.5, 0.4])), frameon=False)\n",
    "        if(self.visited_dict['monster_pos'] == 0):\n",
    "            ax.add_artist(monster)\n",
    "        \n",
    "        goal = AnnotationBbox(OffsetImage(plt.imread('./images/goal.png'), zoom=0.12),  # Plotting the goal.\n",
    "                       list(map(add, self.final_goal_pos, [0.5, 0.5])), frameon=False)\n",
    "        if(self.visited_dict['final_goal_pos'] == 0):\n",
    "            ax.add_artist(goal)\n",
    "        \n",
    "        intermediate_goal1 = AnnotationBbox(OffsetImage(plt.imread('./images/small_reward.png'), zoom=0.09),  # Plotting the intermediate reward.\n",
    "                       list(map(add, self.intermediate_goal1, [0.5, 0.5])), frameon=False)\n",
    "        if(self.visited_dict['intermediate_goal1'] == 0):\n",
    "            ax.add_artist(intermediate_goal1)\n",
    "        \n",
    "        \n",
    "        intermediate_goal3 = AnnotationBbox(OffsetImage(plt.imread('./images/small_reward.png'), zoom=0.09),  # Plotting the intermediate reward.\n",
    "                       list(map(add, self.intermediate_goal3, [0.5, 0.5])), frameon=False)\n",
    "        if(self.visited_dict['intermediate_goal3'] == 0):\n",
    "            ax.add_artist(intermediate_goal3)\n",
    "        \n",
    "        intermediate_goal2 = AnnotationBbox(OffsetImage(plt.imread('./images/big_reward.png'), zoom=0.09),  # Plotting the intermediate reward.\n",
    "                       list(map(add, self.intermediate_goal2, [0.5, 0.5])), frameon=False)\n",
    "        if(self.visited_dict['intermediate_goal2'] == 0):\n",
    "            ax.add_artist(intermediate_goal2)\n",
    "        \n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     type1 = 'deterministic'\n",
    "#     type2 = 'stochastic'\n",
    "    \n",
    "#     env = GridEnvironment(type1) #to run in deterministic env pass type1 in parameter\n",
    "#     obs = env.reset()\n",
    "#     env.max_timesteps = 10\n",
    "#     while(True):\n",
    "#         action = random.randint(0,3)  #randomly taking any of the actions\n",
    "#         print('Timestep: {}'.format(env.timestep))\n",
    "#         print('Action to be taken: {}'.format(env.action_values(action)))\n",
    "#         observation, reward,done, info = env.step(action)\n",
    "#         print('observation: ',observation)\n",
    "#         print('Current Action Reward: {}, Total Reward:{}'.format(reward, env.reward))\n",
    "\n",
    "#         env.render()\n",
    "#         if(done):\n",
    "#             if(info['next_action_possible'] == -2):\n",
    "#                 print('Oops! Agent was eaten by Monster')\n",
    "#             if(info['next_action_possible'] == -1):\n",
    "#                 print('Oops! Agent fell into the pit')\n",
    "#             if(info['next_action_possible'] == 0):\n",
    "#                 print('Congratulations! Agent reached goal in time: {}'.format(env.timestep + 1))\n",
    "#             if(info['next_action_possible'] == 1):\n",
    "#                 print('Oops! Agent ran out of timesteps')\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.pointer = 0\n",
    "        self.experience = []\n",
    "    \n",
    "    def store_experience(self, experience):\n",
    "        self.experience[self.pointer] = experience\n",
    "        \n",
    "    def sample(self, batch):\n",
    "        indexes = random.sample(self.experience, batch)\n",
    "        #get samples for indexes\n",
    "        \n",
    "    # sample sample\n",
    "    # sample sample V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
