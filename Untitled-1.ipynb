{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.random.random((5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.73635991, 0.22125003, 0.69623477, 0.89401721, 0.52962354],\n",
       "       [0.8521962 , 0.21119063, 0.5499907 , 0.97250887, 0.42481825],\n",
       "       [0.63951763, 0.59505719, 0.991069  , 0.60938327, 0.13284453],\n",
       "       [0.15223943, 0.82866268, 0.7528852 , 0.404734  , 0.30175281],\n",
       "       [0.18465253, 0.14547549, 0.17460853, 0.89189669, 0.57518932]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73635991 0.22125003 0.69623477 0.89401721 0.52962354 0.8521962\n",
      " 0.21119063 0.5499907  0.97250887 0.42481825 0.63951763 0.59505719\n",
      " 0.991069   0.60938327 0.13284453 0.15223943 0.82866268 0.7528852\n",
      " 0.404734   0.30175281 0.18465253 0.14547549 0.17460853 0.89189669\n",
      " 0.57518932]\n"
     ]
    }
   ],
   "source": [
    "print(array.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from matplotlib.offsetbox import AnnotationBbox, OffsetImage\n",
    "from operator import add\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "plt.rcParams[\"figure.figsize\"] = (20,15)\n",
    "\n",
    "class GridEnvironment(gym.Env):\n",
    "    \n",
    "    def __init__(self, type):                     #type variable defines the environment type ie deterministic or stochastic\n",
    "        self.observation_space = spaces.Discrete(25)\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.max_timesteps = 25\n",
    "        self.reward_action_step = -0.5          #reward associated with each timestep\n",
    "        self.outbound_reward = -1             #each time the agent tries to go out of the environment it gets negative reward\n",
    "        self.env_type = type\n",
    "        print('***************{}***************'.format(self.env_type+' environment'))\n",
    "        \n",
    "    def reset(self):\n",
    "        self.timestep = 0\n",
    "        self.reward = 0\n",
    "        self.done = False\n",
    "        self.agent_pos = [0,0]\n",
    "        self.intermediate_goal1 = [2,1]\n",
    "        self.intermediate_goal2 = [0,4]\n",
    "        self.intermediate_goal3 = [4,0]\n",
    "        self.final_goal_pos = [4,3]\n",
    "        self.monster_pos = [1,4]\n",
    "        self.pit_pos = [3,1]\n",
    "        self.state = np.zeros((5, 5))\n",
    "        \n",
    "        self.state[tuple(self.agent_pos)] = 1\n",
    "        observation = self.state.flatten()\n",
    "        self.reward_dict = {                         #dict to store the values of rewards\n",
    "            'intermediate_goal1': 2,\n",
    "            'intermediate_goal2': 0.5,\n",
    "            'intermediate_goal3': 2,\n",
    "            'final_goal': 25,\n",
    "            'monster': -10,\n",
    "            'pit': -5\n",
    "        }\n",
    "        self.visited_dict = {                        #dict to store the visited status \n",
    "            'intermediate_goal1': 0,\n",
    "            'intermediate_goal2': 0,\n",
    "            'intermediate_goal3': 0,\n",
    "            'final_goal_pos': 0,\n",
    "            'monster_pos': 0,\n",
    "            'pit_pos': 0\n",
    "        }\n",
    "        \n",
    "        return observation\n",
    "    \n",
    "    def step(self,action):\n",
    "\n",
    "        current_reward = self.reward_action_step\n",
    "        self.timestep += 1\n",
    "        \n",
    "        #defining the stochastic part of the environment\n",
    "        if(self.env_type == 'stochastic'):\n",
    "            if(action == 0):\n",
    "                action = np.random.choice(4, 1, p=[0.95, 0.03, 0.01, 0.01])[0]\n",
    "            elif(action == 1):\n",
    "                action = np.random.choice(4, 1, p=[0.03, 0.95, 0.01, 0.01])[0]\n",
    "            elif(action == 2):\n",
    "                action = np.random.choice(4, 1, p=[0.01, 0.01, 0.95, 0.03])[0]\n",
    "            else:\n",
    "                action = np.random.choice(4, 1, p=[0.01, 0.01, 0.03, 0.95])[0]\n",
    "                \n",
    "#         print('Action that happened: {}'.format(self.action_values(action)))\n",
    "\n",
    "        old_pos = self.agent_pos.copy()\n",
    "        print(\"{} old pos is\".format(old_pos))\n",
    "        if action == 0:\n",
    "            self.agent_pos[1] -= 1 #down\n",
    "        if action == 1:\n",
    "            self.agent_pos[1] += 1 #up\n",
    "        if action == 2:\n",
    "            self.agent_pos[0] -= 1 #left\n",
    "        if action == 3:\n",
    "            self.agent_pos[0] += 1 #right\n",
    "        \n",
    "        if(self.agent_pos[0] > 4 or self.agent_pos[1] > 4 or self.agent_pos[0] < 0 or self.agent_pos[1] < 0):\n",
    "            current_reward += self.outbound_reward\n",
    "        self.agent_pos = np.clip(self.agent_pos, 0, 4)         #clip function to ensure safety of the agent\n",
    "        self.state = np.zeros((5,5))\n",
    "        \n",
    "        next_action_possible = 1                            #defines whether the next action is possible or not\n",
    "                                                            # 1 if possible 0 if goal reached and -1 if dead by monster or in pit\n",
    "        \n",
    "        if(self.agent_pos == self.intermediate_goal1).all():\n",
    "            if(self.visited_dict['intermediate_goal1'] == 0):\n",
    "                current_reward += self.reward_dict['intermediate_goal1']\n",
    "            self.visited_dict['intermediate_goal1'] += 1\n",
    "            \n",
    "        if(self.agent_pos == self.intermediate_goal2).all():\n",
    "            if(self.visited_dict['intermediate_goal2'] == 0):\n",
    "                current_reward += self.reward_dict['intermediate_goal2']\n",
    "            self.visited_dict['intermediate_goal2'] += 1\n",
    "            \n",
    "        if(self.agent_pos == self.intermediate_goal3).all():\n",
    "            if(self.visited_dict['intermediate_goal3'] == 0):\n",
    "                current_reward += self.reward_dict['intermediate_goal3']\n",
    "            self.visited_dict['intermediate_goal3'] += 1\n",
    "        \n",
    "        if (self.agent_pos == self.final_goal_pos).all():\n",
    "            current_reward += self.reward_dict['final_goal']\n",
    "            next_action_possible = 0\n",
    "            self.visited_dict['final_goal_pos'] += 1\n",
    "        \n",
    "        self.done = True if(self.timestep >= self.max_timesteps or (self.agent_pos == self.final_goal_pos).all())else False\n",
    "        \n",
    "        if(self.agent_pos == self.monster_pos).all():\n",
    "            self.visited_dict['monster_pos'] += 1\n",
    "            current_reward += self.reward_dict['monster']\n",
    "            self.done = True\n",
    "            next_action_possible = -2\n",
    "        \n",
    "        if(self.agent_pos == self.pit_pos).all():\n",
    "            self.visited_dict['pit_pos'] += 1\n",
    "            current_reward += self.reward_dict['pit']\n",
    "            self.done = True\n",
    "            next_action_possible = -1\n",
    "        \n",
    "        self.reward += current_reward\n",
    "        info = {'next_action_possible': next_action_possible, 'current_agent_pos': self.agent_pos}\n",
    "        self.state = np.zeros((5, 5))\n",
    "        self.state[tuple(self.agent_pos)] = 1\n",
    "        observation = self.state.flatten()\n",
    "        return observation, current_reward, self.done, info\n",
    "    \n",
    "    def action_values(self, action):                                      #method to give the name of the action\n",
    "        a = ''\n",
    "        if(action == 0):\n",
    "            a = 'Down'\n",
    "        if(action == 1):\n",
    "            a = 'Up'\n",
    "        if(action == 2):\n",
    "            a = 'Left'\n",
    "        if(action == 3):\n",
    "            a = 'Right'\n",
    "        return a\n",
    "    \n",
    "    def render(self):\n",
    "        fix ,ax = plt.subplots(figsize=(10,10))\n",
    "        ax.set_xlim(0,5)\n",
    "        ax.set_ylim(0,5)\n",
    "\n",
    "        agent = AnnotationBbox(OffsetImage(plt.imread('./images/agent.png'), zoom=0.20),  # Plotting the agent.\n",
    "                       list(map(add, self.agent_pos, [0.5, 0.5])), frameon=False)\n",
    "        if(self.agent_pos[0] == self.intermediate_goal1[0] and self.agent_pos[1] == self.intermediate_goal1[1]):\n",
    "            if(self.visited_dict['intermediate_goal1'] == 1):\n",
    "                agent = AnnotationBbox(OffsetImage(plt.imread('./images/agent_reward.png'), zoom=0.15),  # Plotting the agent with reward.\n",
    "                           list(map(add, self.agent_pos, [0.5, 0.5])), frameon=False)\n",
    "        elif(self.agent_pos[0] == self.intermediate_goal3[0] and self.agent_pos[1] == self.intermediate_goal3[1]):\n",
    "            if(self.visited_dict['intermediate_goal3'] == 1):\n",
    "                agent = AnnotationBbox(OffsetImage(plt.imread('./images/agent_reward.png'), zoom=0.15),  # Plotting the agent with reward.\n",
    "                           list(map(add, self.agent_pos, [0.5, 0.5])), frameon=False)\n",
    "        elif(self.agent_pos[0] == self.intermediate_goal2[0] and self.agent_pos[1] == self.intermediate_goal2[1]):\n",
    "            if(self.visited_dict['intermediate_goal2'] == 1):\n",
    "                agent = AnnotationBbox(OffsetImage(plt.imread('./images/agent_reward.png'), zoom=0.15),  # Plotting the agent with reward.\n",
    "                           list(map(add, self.agent_pos, [0.5, 0.5])), frameon=False)\n",
    "        elif(self.agent_pos[0] == self.pit_pos[0] and self.agent_pos[1] == self.pit_pos[1]):\n",
    "            agent = AnnotationBbox(OffsetImage(plt.imread('./images/agent_in_pit.png'), zoom=0.08),  # Plotting the agent in pit.\n",
    "                       list(map(add, self.agent_pos, [0.5, 0.5])), frameon=False)\n",
    "        elif(self.agent_pos[0] == self.final_goal_pos[0] and self.agent_pos[1] == self.final_goal_pos[1]):\n",
    "            agent = AnnotationBbox(OffsetImage(plt.imread('./images/agent_goal.png'), zoom=0.18),  # Plotting the agent with goal.\n",
    "                       list(map(add, self.agent_pos, [0.5, 0.5])), frameon=False)\n",
    "        elif(self.agent_pos[0] == self.monster_pos[0] and self.agent_pos[1] == self.monster_pos[1]):\n",
    "            agent = AnnotationBbox(OffsetImage(plt.imread('./images/dead_agent.png'), zoom=0.18),  # Plotting the dead agent.\n",
    "                       list(map(add, self.agent_pos, [0.5, 0.5])), frameon=False)\n",
    "        ax.add_artist(agent)\n",
    "        \n",
    "        pit = AnnotationBbox(OffsetImage(plt.imread('./images/pit.png'), zoom=0.10),  # Plotting the pit.\n",
    "                       list(map(add, self.pit_pos, [0.5, 0.5])), frameon=False)\n",
    "        if(self.visited_dict['pit_pos'] == 0):\n",
    "            ax.add_artist(pit)\n",
    "        \n",
    "        monster = AnnotationBbox(OffsetImage(plt.imread('./images/monster.png'), zoom=0.12),  # Plotting the monster.\n",
    "                       list(map(add, self.monster_pos, [0.5, 0.4])), frameon=False)\n",
    "        if(self.visited_dict['monster_pos'] == 0):\n",
    "            ax.add_artist(monster)\n",
    "        \n",
    "        goal = AnnotationBbox(OffsetImage(plt.imread('./images/goal.png'), zoom=0.12),  # Plotting the goal.\n",
    "                       list(map(add, self.final_goal_pos, [0.5, 0.5])), frameon=False)\n",
    "        if(self.visited_dict['final_goal_pos'] == 0):\n",
    "            ax.add_artist(goal)\n",
    "        \n",
    "        intermediate_goal1 = AnnotationBbox(OffsetImage(plt.imread('./images/small_reward.png'), zoom=0.09),  # Plotting the intermediate reward.\n",
    "                       list(map(add, self.intermediate_goal1, [0.5, 0.5])), frameon=False)\n",
    "        if(self.visited_dict['intermediate_goal1'] == 0):\n",
    "            ax.add_artist(intermediate_goal1)\n",
    "        \n",
    "        \n",
    "        intermediate_goal3 = AnnotationBbox(OffsetImage(plt.imread('./images/small_reward.png'), zoom=0.09),  # Plotting the intermediate reward.\n",
    "                       list(map(add, self.intermediate_goal3, [0.5, 0.5])), frameon=False)\n",
    "        if(self.visited_dict['intermediate_goal3'] == 0):\n",
    "            ax.add_artist(intermediate_goal3)\n",
    "        \n",
    "        intermediate_goal2 = AnnotationBbox(OffsetImage(plt.imread('./images/big_reward.png'), zoom=0.09),  # Plotting the intermediate reward.\n",
    "                       list(map(add, self.intermediate_goal2, [0.5, 0.5])), frameon=False)\n",
    "        if(self.visited_dict['intermediate_goal2'] == 0):\n",
    "            ax.add_artist(intermediate_goal2)\n",
    "        \n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************deterministic environment***************\n"
     ]
    }
   ],
   "source": [
    "env = GridEnvironment('deterministic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "obv = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " -0.5,\n",
       " False,\n",
       " {'next_action_possible': 1, 'current_agent_pos': array([1, 0])})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]).reshape((5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp2 = np.array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]).reshape((5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4e5a298e8f6c08cb1c67119fb54864d83c3a44710d4d324d683ac8465221c20b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
